{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cf4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d495c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_delay_embed(x: np.ndarray, l: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    l: Window.\n",
    "    x: Time Series.\n",
    "    \"\"\"\n",
    "    if l < 1:\n",
    "        raise ValueError(\"Embedding length l must be >= 1.\")\n",
    "    \n",
    "    T = x.shape[0] # time series length\n",
    "    if T < l:\n",
    "        raise ValueError(f\"Series length {T} is shorter than embedding l={l}.\")\n",
    "\n",
    "    from numpy.lib.stride_tricks import as_strided\n",
    "    n_rows = T - l + 1\n",
    "    stride = x.strides[0] # Location in memory \n",
    "    X = as_strided(x, shape=(n_rows, l), strides=(stride, stride))\n",
    "    return X.copy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mahalanobis_VI(T: np.ndarray, reg: float = 1e-6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute inverse covariance (VI) for Mahalanobis distance on sample T (n x l).\n",
    "    Adds Tikhonov regularization reg*I for stability.\n",
    "    \"\"\"\n",
    "    l = T.shape[1]\n",
    "    cov = np.cov(T, rowvar=False)\n",
    "    if cov.ndim == 0:  # l == 1 edge case: scalar var\n",
    "        cov = np.array([[cov]])\n",
    "    cov = cov + reg * np.eye(l)\n",
    "    # Pseudoinverse is safer than inverse\n",
    "    VI = np.linalg.pinv(cov)\n",
    "    return VI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02140feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _mahalanobis_distances(T: np.ndarray, x: np.ndarray, VI: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Vectorized Mahalanobis distances from x to each row in T.\n",
    "    \"\"\"\n",
    "    diff = T - x  # (n, l)\n",
    "    # sqrt( (x-y)^T VI (x-y) ) for each row\n",
    "    d2 = np.einsum('ij,jk,ik->i', diff, VI, diff, optimize=True)\n",
    "    d2 = np.maximum(d2, 0.0)  # guard tiny negatives from numerics\n",
    "    return np.sqrt(d2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4373a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_nonconformity(T: np.ndarray,\n",
    "                      x: np.ndarray,\n",
    "                      k: int,\n",
    "                      metric: str = \"mahalanobis\",\n",
    "                      VI: Optional[np.ndarray] = None) -> float:\n",
    "    \"\"\"\n",
    "    Average distance from x to its k nearest neighbors in T (excluding x itself; x is not in T here).\n",
    "    metric: 'mahalanobis' (default) or 'euclidean'.\n",
    "    For 'mahalanobis', pass VI (inverse covariance) for T, or it will be computed.\n",
    "    \"\"\"\n",
    "    if T.ndim != 2 or x.ndim != 1:\n",
    "        raise ValueError(\"Shapes must be T:(n,l), x:(l,).\")\n",
    "    n = T.shape[0]\n",
    "    if n == 0:\n",
    "        return np.nan\n",
    "    k_eff = max(1, min(k, n))\n",
    "    if metric == \"mahalanobis\":\n",
    "        if VI is None:\n",
    "            VI = _mahalanobis_VI(T)\n",
    "        d = _mahalanobis_distances(T, x, VI)\n",
    "    elif metric == \"euclidean\":\n",
    "        diff = T - x\n",
    "        d = np.sqrt(np.sum(diff * diff, axis=1))\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'mahalanobis' or 'euclidean'\")\n",
    "    # take k smallest distances\n",
    "    idx = np.argpartition(d, k_eff - 1)[:k_eff]\n",
    "    return float(np.mean(d[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class LDCDConfig:\n",
    "    l: int = 19           # embedding dimension\n",
    "    k: int = 27           # neighbors\n",
    "    n: int = 600          # training window (embedded points)\n",
    "    m: int = 600          # calibration window (scores)\n",
    "    metric: str = \"mahalanobis\"  # or 'euclidean'\n",
    "    reg: float = 1e-6     # covariance regularization\n",
    "    prune: bool = False   # alarm thinning\n",
    "    prune_quantile: float = 0.995\n",
    "    cooldown_frac: float = 0.2  # n/5 as in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079131b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ldcd_conformal_knn(x: np.ndarray, cfg: LDCDConfig) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Run LDCD on univariate series x. Returns dict with arrays aligned to original time index.\n",
    "    Keys:\n",
    "      'anomaly_prob' : 1 - conformal p-values (NaN before first valid index)\n",
    "      'p_value'      : conformal p-values (NaN before first valid index)\n",
    "      'score'        : kNN nonconformity scores (embedded-time aligned)\n",
    "      'valid_from'   : integer (raw index) where the first p-value is defined\n",
    "      'offset'       : embedding offset (= l-1)\n",
    "    \"\"\"\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"x must be a 1-D array.\")\n",
    "    l, k, n, m = cfg.l, cfg.k, cfg.n, cfg.m\n",
    "    if any(v < 1 for v in [l, k, n, m]):\n",
    "        raise ValueError(\"l, k, n, m must all be >= 1\")\n",
    "\n",
    "    # 1) Embed\n",
    "    X = time_delay_embed(x, l)                  # shape (N_emb, l)\n",
    "    N_emb = X.shape[0]\n",
    "    offset = l - 1  # raw index for embedded row t is t + offset\n",
    "\n",
    "    # Check we have enough embedded samples\n",
    "    if N_emb < n + m + 1:\n",
    "        raise ValueError(f\"Not enough data. Need at least l + n + m points: \"\n",
    "                         f\"l={l} n={n} m={m} -> min raw length {l + n + m}\")\n",
    "\n",
    "    # Output arrays (embedded timeline)\n",
    "    scores = np.full(N_emb, np.nan, dtype=float)     # nonconformity\n",
    "    pvals  = np.full(N_emb, np.nan, dtype=float)     # conformal p-values\n",
    "    aprobs = np.full(N_emb, np.nan, dtype=float)     # anomaly probability = 1 - p\n",
    "\n",
    "    # 2) Initialize training block at t0 = n + m - 1 (embedded index), training indices [0, n-1]\n",
    "    # Fill initial calibration queue with scores for s in [n, n+m-1] scored against that training block.\n",
    "    t0 = n + m - 1\n",
    "    T0 = X[0:n]  # (n, l)\n",
    "\n",
    "    # Precompute VI if using Mahalanobis for the initial block\n",
    "    VI0 = _mahalanobis_VI(T0, reg=cfg.reg) if cfg.metric == \"mahalanobis\" else None\n",
    "\n",
    "    for s in range(n, n + m):\n",
    "        scores[s] = knn_nonconformity(T0, X[s], k, metric=cfg.metric, VI=VI0)\n",
    "\n",
    "    # 3) Main loop: starting at t = n + m (first index with m prior scores available)\n",
    "    cooldown = 0\n",
    "    for t in range(n + m, N_emb):\n",
    "        # Build training window T_t: indices [t - (n + m), t - m - 1]  (length n)\n",
    "        start = t - (n + m)\n",
    "        end = t - m  # exclusive\n",
    "        T_t = X[start:end]  # shape (n, l)\n",
    "        if cfg.metric == \"mahalanobis\":\n",
    "            VI_t = _mahalanobis_VI(T_t, reg=cfg.reg)\n",
    "        else:\n",
    "            VI_t = None\n",
    "\n",
    "        # Nonconformity for current x_t against T_t\n",
    "        a_t = knn_nonconformity(T_t, X[t], k, metric=cfg.metric, VI=VI_t)\n",
    "        scores[t] = a_t\n",
    "\n",
    "        # Calibration scores: last m scores (a_{t-m}..a_{t-1})\n",
    "        cal = scores[t - m: t]\n",
    "        if np.any(np.isnan(cal)):\n",
    "            # Should not happen after initialization, but be safe\n",
    "            valid = cal[~np.isnan(cal)]\n",
    "            if valid.size < m:\n",
    "                # Not enough calibration yet\n",
    "                pvals[t] = np.nan\n",
    "                aprobs[t] = np.nan\n",
    "                continue\n",
    "            cal = valid[-m:]\n",
    "\n",
    "        # Conformal p-value (LDCD): (count of >= including current) / (m+1)\n",
    "        ge = np.sum(cal >= a_t)  # number of previous >= current\n",
    "        p = (ge + 1) / float(m + 1)\n",
    "        pvals[t] = p\n",
    "        anomaly_prob = 1.0 - p\n",
    "\n",
    "        # Optional pruning: if high alarm, set next n/5 outputs to 0.5\n",
    "        if cfg.prune and anomaly_prob > cfg.prune_quantile and cooldown <= 0:\n",
    "            cooldown = max(1, int(round(cfg.cooldown_frac * n)))\n",
    "\n",
    "        if cooldown > 0:\n",
    "            aprobs[t] = 0.5\n",
    "            cooldown -= 1\n",
    "        else:\n",
    "            aprobs[t] = anomaly_prob\n",
    "\n",
    "    # Map to raw time axis (length len(x)), with NaNs before offset\n",
    "    anomaly_prob_raw = np.full_like(x, np.nan, dtype=float)\n",
    "    pvals_raw = np.full_like(x, np.nan, dtype=float)\n",
    "    # Fill from embedded indices where p-values are defined (t >= n+m)\n",
    "    first_valid_emb = n + m\n",
    "    if first_valid_emb < N_emb:\n",
    "        raw_idx = np.arange(first_valid_emb, N_emb) + offset\n",
    "        anomaly_prob_raw[raw_idx] = aprobs[first_valid_emb:N_emb]\n",
    "        pvals_raw[raw_idx] = pvals[first_valid_emb:N_emb]\n",
    "\n",
    "    return dict(\n",
    "        anomaly_prob=anomaly_prob_raw,\n",
    "        p_value=pvals_raw,\n",
    "        score=scores,           # note: embedded timeline\n",
    "        valid_from=int(first_valid_emb + offset),\n",
    "        offset=int(offset)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b2862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement a Python script that applies the LDCD conformal k-NN anomaly detector\n",
    "# on a univariate time-series. We'll match the paper's method exactly.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class LDCD_KNN:\n",
    "    def __init__(self, l=19, k=27, n=100, m=50):\n",
    "        \"\"\"\n",
    "        l: embedding dimension (time-delay window length)\n",
    "        k: number of nearest neighbors\n",
    "        n: training set size\n",
    "        m: calibration set size\n",
    "        \"\"\"\n",
    "        self.l = l\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.N = n + m\n",
    "        self.training_set = None\n",
    "        self.calibration_scores = None\n",
    "        self.is_initialized = False\n",
    "\n",
    "    def _embed(self, series):\n",
    "        \"\"\"Embed univariate series into l-dim time-delay vectors.\"\"\"\n",
    "        T = len(series)\n",
    "        embedded = np.zeros((T - self.l + 1, self.l))\n",
    "        for i in range(T - self.l + 1):\n",
    "            embedded[i] = series[i:i + self.l]\n",
    "        return embedded\n",
    "\n",
    "    def _knn_score(self, point, reference):\n",
    "        \"\"\"Compute average distance to k nearest neighbors.\"\"\"\n",
    "        neigh = NearestNeighbors(n_neighbors=min(self.k, len(reference)))\n",
    "        neigh.fit(reference)\n",
    "        distances, _ = neigh.kneighbors([point])\n",
    "        return np.mean(distances)\n",
    "\n",
    "    def initialize(self, embedded_series):\n",
    "        \"\"\"Initialize training and calibration sets.\"\"\"\n",
    "        self.training_set = embedded_series[:self.n]\n",
    "        self.calibration_scores = []\n",
    "        for idx in range(self.n, self.N):\n",
    "            score = self._knn_score(embedded_series[idx], self.training_set)\n",
    "            self.calibration_scores.append(score)\n",
    "        self.is_initialized = True\n",
    "\n",
    "    def update(self, new_point):\n",
    "        \"\"\"Process a new embedded point and return anomaly score.\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            raise RuntimeError(\"LDCD not initialized.\")\n",
    "        # Compute score using current training set\n",
    "        score = self._knn_score(new_point, self.training_set)\n",
    "        # Compute conformal p-value\n",
    "        count = sum(1 for s in self.calibration_scores if s >= score)\n",
    "        p_value = (count + 1) / (self.m + 1)  # +1 includes current score\n",
    "        anomaly_score = 1 - p_value\n",
    "        # Update calibration queue\n",
    "        self.calibration_scores.pop(0)\n",
    "        self.calibration_scores.append(score)\n",
    "        # Update training set (slide forward)\n",
    "        self.training_set = np.vstack([self.training_set[1:], new_point])\n",
    "        return anomaly_score\n",
    "\n",
    "    def fit_predict(self, series):\n",
    "        \"\"\"Fit LDCD to a univariate series and return anomaly scores.\"\"\"\n",
    "        embedded = self._embed(series)\n",
    "        scores = [None] * len(series)\n",
    "        self.initialize(embedded)\n",
    "        # Fill scores for burn-in period with NaN\n",
    "        for t in range(self.N + self.l - 1):\n",
    "            scores[t] = np.nan\n",
    "        # Process the rest\n",
    "        for idx in range(self.N, len(embedded)):\n",
    "            s = self.update(embedded[idx])\n",
    "            scores[idx + self.l - 1] = s\n",
    "        return scores\n",
    "\n",
    "# Example usage with synthetic data\n",
    "if __name__ == \"__main__\":\n",
    "    # Create synthetic data with anomalies\n",
    "    np.random.seed(0)\n",
    "    normal_part = np.sin(np.linspace(0, 20, 300)) + 0.1 * np.random.randn(300)\n",
    "    anomalies = np.random.uniform(3, 4, size=10)\n",
    "    series = np.copy(normal_part)\n",
    "    anomaly_positions = np.random.choice(len(series), size=10, replace=False)\n",
    "    series[anomaly_positions] = anomalies\n",
    "\n",
    "    # Instantiate and run LDCD\n",
    "    detector = LDCD_KNN(l=19, k=27, n=100, m=50)\n",
    "    scores = detector.fit_predict(series)\n",
    "\n",
    "    import pandas as pd\n",
    "    df_results = pd.DataFrame({\n",
    "        \"value\": series,\n",
    "        \"anomaly_score\": scores,\n",
    "        \"is_anomaly\": [1 if i in anomaly_positions else 0 for i in range(len(series))]\n",
    "    })\n",
    "\n",
    "    import ace_tools as tools; tools.display_dataframe_to_user(name=\"LDCD Anomaly Detection Results\", dataframe=df_results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
